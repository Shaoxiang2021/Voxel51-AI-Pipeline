{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f8cec4",
   "metadata": {},
   "source": [
    "# Loading a Segmentation Dataset\n",
    "\n",
    "In this first step, we will explore how to load segmentation datasets into FiftyOne. Segmentation datasets may be of two types: semantic segmentation (pixel-wise class labels) and instance segmentation (individual object masks).\n",
    "\n",
    "FiftyOne makes it easy to load both types using its Dataset Zoo or from custom formats like COCO or FiftyOne format. Letâ€™s start by loading a common format instance segmentation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd015de9",
   "metadata": {},
   "source": [
    "## Loading a Common Format Segmentation Dataset\n",
    "\n",
    "Segmentation datasets are often provided in standard formats such as COCO, VOC, YOLO, KITTI, and FiftyOne format. FiftyOne supports direct ingestion of these datasets with just a few lines of code.\n",
    "\n",
    "Make sure your dataset follows the folder structure and file naming conventions required by the specific format (e.g., COCO JSON annotations or class mask folders for semantic segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9861ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Create the dataset\n",
    "name = \"mercedes-dataset\"\n",
    "dataset_dir = \"../datasets/mercedes\"  # Change with your path\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.COCODetectionDataset, # Change with your type\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab273f",
   "metadata": {},
   "source": [
    "Check out the docs for each format to find optional parameters you can pass for things like train/test split, subfolders, or label paths, check more in the User Guide of Using Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf263b7",
   "metadata": {},
   "source": [
    "## FiftyOne with a Coffee-Beans Dataset\n",
    "\n",
    "We will walk through how to use FiftyOne to build better segmentation datasets and models.\n",
    "\n",
    "- Load your own dataset into FiftyOne. For this example, we use a Coffee-Beans Dataset in COCO format.\n",
    "- Use FiftyOne in a notebook\n",
    "- Explore your segmentation dataset using views and the FiftyOne App\n",
    "\n",
    "Note: To load the dataset locally, visit the Coffee-Beans Dataset page on Hugging Face, download the files, and then load them using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e14181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/datasets/pjramg/colombian_coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52be0f",
   "metadata": {},
   "source": [
    "If you only see small pointer files instead of the actual images, it means Git LFS wasnâ€™t used. In that case, use Git LFS to pull the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afded04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    dataset_dir=\"./colombian_coffee\",\n",
    "    data_path=\"images/default\",\n",
    "    labels_path=\"annotations/instances_default.json\",\n",
    "    label_types=\"segmentations\",\n",
    "    label_field=\"categories\",\n",
    "    name=\"coffee\",\n",
    "    include_id=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc61af",
   "metadata": {},
   "source": [
    "We can see our images have loaded in the App, but no segmentation masks are shown yet. Next, weâ€™ll ensure annotations are properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d65401",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aaf5e0",
   "metadata": {},
   "source": [
    "With the FiftyOne App, you can visualize your samples and their segmentation masks in an interactive UI. Double-click any sample to enter the expanded view, where you can study individual samples with overlayed masks.\n",
    "\n",
    "The view bar lets you filter and search your dataset to analyze specific classes or objects.\n",
    "\n",
    "You can seamlessly move between Python and the App. For example, create a filtered view using the Shuffle() and Limit() stages in Python or directly in the App UI.\n",
    "\n",
    "Once your annotations are loaded correctly, you can confirm that your segmentation masks (not detections!) are present and visualized correctly. ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cefe51",
   "metadata": {},
   "source": [
    "# Step 2: Adding Instance Segmentation to a FiftyOne Dataset\n",
    "\n",
    "We will explore how to enrich your dataset by adding instance segmentation predictions.\n",
    "\n",
    "In this notebook, weâ€™ll cover:\n",
    "\n",
    "- Using the FiftyOne Model Zoo to apply instance segmentation\n",
    "- Integrating predictions from a custom model (e.g., a model deployed via Intel Geti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb349b3a",
   "metadata": {},
   "source": [
    "## Using a Instance Segmentation Dataset\n",
    "\n",
    "For education purposes, use this link in Drive for downloading an upgraded dataset with 100+ annotated unique images.\n",
    "\n",
    "Download the dataset with this Link and unzip in your work folder.\n",
    "\n",
    "Letâ€™s kick things off by loading the colombian_coffee-dataset_1600: (This is a new dataset, different from the one used in the last notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    dataset_dir=\"./colombian_coffee-dataset_1600\",\n",
    "    data_path=\"images/default\",\n",
    "    labels_path=\"annotations/instances_default.json\",\n",
    "    label_types=\"segmentations\",\n",
    "    label_field=\"ground_truth\",\n",
    "    name=\"coffe_1600\",\n",
    "    include_id=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "view = dataset.shuffle()\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f165ed1",
   "metadata": {},
   "source": [
    "## Loading predictions using SAM2\n",
    "\n",
    "With FiftyOne, you have tons of pretrained models at your disposal to use via the FiftyOne Model Zoo or using one of our integrations such as HuggingFace! To get started using them, first load the model in and pass it into the apply_model function.\n",
    "\n",
    "Install SAM2 following the instuctions from this Repo. You can also jump to the next step of this tutorials to understand how SAM2 works with FiftyOne\n",
    "\n",
    "https://github.com/facebookresearch/sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install \"sam2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0485753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'git+https://github.com/facebookresearch/sam2.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37aca0",
   "metadata": {},
   "source": [
    "If you encounter any issues, please refer to the main SAM2 repository to verify the installation process Repo.\n",
    "\n",
    "Now apply Segment Anything SAM2 from the FiftyOne Model Zoo. As you can see, some images in the dataset include ground truth annotations, but not all of them. With SAM2, we will apply segmentation across the entire dataset. (This could take around 1.5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "\n",
    "# Prompt with boxes\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"sam2_predictions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd859b3f",
   "metadata": {},
   "source": [
    "Alternatively, you can apply SAM only to the images that already have ground truth segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5822131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"sam2_predictions\",\n",
    "    prompt_field=\"ground_truth_segmentations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377fdf2",
   "metadata": {},
   "source": [
    "This will execute SAM only for images in the segmentation category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b30e27",
   "metadata": {},
   "source": [
    "## Loading predictions using a custom model (Intel Geti Example)\n",
    "\n",
    "Letâ€™s now simulate the pipeline with a custom instance segmentation model. If you want to run the inference using the same example, please refer tho this example for your reference.\n",
    "\n",
    "Assuming youâ€™ve already set up inference with a model (e.g., via OpenVINO + Intel Geti SDK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a602c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geti-sdk==2.10.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbda12",
   "metadata": {},
   "source": [
    "## Preparing the models for inference\n",
    "\n",
    "The Intel Geti SDK will be used to run inference with Intel Geti Models. The deployment folder of the best model must be downloaded and unzipped in the same folder as the project.\n",
    "\n",
    "Download and unzip the model\n",
    "\n",
    "## Generating instance segmentation masks from polygons and bounding boxes\n",
    "This function extracts instance segmentation masks from polygon annotations, combining detection (bounding boxes) and segmentation (masks) in the same instance using fo.Detection.\n",
    "\n",
    "1. Load Image â€“ Reads and converts the image to RGB.\n",
    "2. Process Annotations â€“ Extracts polygon points, computes bounding boxes, and normalizes coordinates.\n",
    "3. Generate Masks â€“ Creates, crops, and resizes binary masks for each annotation.\n",
    "4. Save & Return â€“ Stores masks as temp files and returns fo.Detection objects, ensuring the bounding box and mask belong to the same instance.\n",
    "\n",
    "This enables accurate visualization and analysis in FiftyOne, preserving both object localization and shape details.\n",
    "\n",
    "Useful for visualizing or processing segmentation data in FiftyOne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "from PIL import Image as PILImage\n",
    "from tempfile import NamedTemporaryFile\n",
    "from geti_sdk.deployment import Deployment\n",
    "from geti_sdk.data_models.shapes import Polygon\n",
    "\n",
    "def generate_mask_from_polygon_and_bboxes(sample, prediction):\n",
    "    image = cv2.imread(sample.filepath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    print(f\"Image size: {img_width}x{img_height}\")\n",
    "    detections = []\n",
    "    for annotation in prediction.annotations:\n",
    "        if isinstance(annotation.shape, Polygon):\n",
    "            polygon_points = [(point.x, point.y) for point in annotation.shape.points]\n",
    "            polygon_points = np.array(polygon_points, dtype=np.int32)\n",
    "            label = annotation.labels[0].name\n",
    "            confidence = annotation.labels[0].probability\n",
    "            x, y, w, h = cv2.boundingRect(polygon_points)\n",
    "            scaled_x = x / img_width\n",
    "            scaled_y = y / img_height\n",
    "            scaled_w = w / img_width\n",
    "            scaled_h = h / img_height\n",
    "            bounding_box = [scaled_x, scaled_y, scaled_w, scaled_h]\n",
    "            mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, [polygon_points], 255)\n",
    "            cropped_mask = mask[y:y + h, x:x + w]\n",
    "            mask_resized = cv2.resize(cropped_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            print(f\"Mask size: {mask_resized.shape} (expected: {h}x{w})\")\n",
    "            with NamedTemporaryFile(delete=False, suffix='.png') as temp_mask_file:\n",
    "                mask_path = temp_mask_file.name\n",
    "                cv2.imwrite(mask_path, mask_resized)\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                confidence=confidence,\n",
    "                bounding_box=bounding_box,\n",
    "                mask_path=mask_path\n",
    "            )\n",
    "            detections.append(detection)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a40882",
   "metadata": {},
   "source": [
    "For education purposes check what is happening in the first or last sample. Then you can apply this to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d32383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the folder path to match the location where the model was downloaded and unzipped\n",
    "deployment_inference = Deployment.from_folder(\"geti_sdk-deployment_90\")\n",
    "deployment_inference.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4baf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one image\n",
    "sample = dataset.first()\n",
    "image_path = sample.filepath\n",
    "image_data = PILImage.open(image_path)\n",
    "image_data = np.array(image_data)\n",
    "prediction = deployment_inference.infer(image_data)\n",
    "detections = generate_mask_from_polygon_and_bboxes(sample, prediction)\n",
    "sample['predicted_segmentations_test'] = fo.Detections(detections=detections)\n",
    "sample.save()\n",
    "dataset.reload()\n",
    "print(dataset)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbb633",
   "metadata": {},
   "source": [
    " Tip: Replace prediction.objects with your real output structure and masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2920c26",
   "metadata": {},
   "source": [
    "## Run the prediction in the whole dataset\n",
    "\n",
    "This loop processes each sample in the dataset by loading the image, running inference using Geti SDK, and generating instance segmentation masks. The function extracts detections with both bounding boxes and masks, ensuring they belong to the same instance. These predictions are then stored in the sample under \"predictions_model\" using fo.Detections. Finally, the dataset is reloaded to reflect the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the samples in the dataset\n",
    "for sample in dataset:\n",
    "    # Load the image as a NumPy array using PIL or OpenCV\n",
    "    image_path = sample.filepath  # Path to the image file\n",
    "    image_data = PILImage.open(image_path)\n",
    "    image_data = np.array(image_data)  # Convert the image to NumPy array\n",
    "\n",
    "    # Run inference on the sample (using Geti SDK's inference)\n",
    "    prediction = deployment_inference.infer(image_data)\n",
    "\n",
    "    # Generate the segmentation mask and detections using the annotations from the prediction\n",
    "    detections = generate_mask_from_polygon_and_bboxes(sample, prediction)\n",
    "\n",
    "    # Add the detections as predicted segmentations\n",
    "    sample[\"predictions_geti_sdk\"] = fo.Detections(detections=detections)\n",
    "\n",
    "    # Save the updated sample\n",
    "    sample.save()\n",
    "\n",
    "# Reload the dataset to reflect the changes\n",
    "dataset.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465de5ae",
   "metadata": {},
   "source": [
    "## Compare Predictions in FiftyOne App\n",
    "\n",
    "Toggle between ground_truth_segmentations, sam2_predictions, and predictions_geti_sdk in the App to explore and compare different segmentations side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf04e58",
   "metadata": {},
   "source": [
    "# Step 3: Using SAM 2\n",
    "\n",
    "Segment Anything 2 (SAM 2) is a powerful segmentation model released in July 2024 that pushes the boundaries of image and video segmentation. It brings new capabilities to computer vision applications, including the ability to generate precise masks and track objects across frames in videos using just simple prompts.\n",
    "\n",
    "In this notebook, youâ€™ll learn how to:\n",
    "\n",
    "- Understand the key innovations in SAM 2\n",
    "- Apply SAM 2 to image datasets using bounding boxes, keypoints, or no prompts at all\n",
    "- Leverage SAM 2â€™s video segmentation and mask tracking capabilities with a single-frame prompt\n",
    "\n",
    "## What is SAM 2?\n",
    "\n",
    "SAM 2 is the next generation of the Segment Anything Model, originally introduced by Meta in 2023. While SAM was designed for zero-shot segmentation on still images, SAM 2 adds robust video segmentation and tracking capabilities. With just a bounding box or a set of keypoints on a single frame, SAM 2 can segment and track objects across entire video sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c6cea",
   "metadata": {},
   "source": [
    "## Using SAM 2 for Images\n",
    "\n",
    "SAM 2 integrates directly with the FiftyOne Model Zoo, allowing you to apply segmentation to image datasets with minimal code. Whether youâ€™re working with ground truth bounding boxes, keypoints, or want to explore automatic mask generation, FiftyOne makes the process seamless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7485ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart-25'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [4.0s elapsed, 0s remaining, 8.7 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [4.0s elapsed, 0s remaining, 8.7 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=e113cbd7-ecde-4c2c-bbbc-7c9db28a448b\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd054231840>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load dataset\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25, shuffle=True, seed=51)\n",
    "\n",
    "# Load SAM 2 image model\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "\n",
    "# Prompt with bounding boxes\n",
    "dataset.apply_model(model, label_field=\"segmentations\", prompt_field=\"ground_truth\")\n",
    "\n",
    "# Launch app to view segmentations\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f89f58",
   "metadata": {},
   "source": [
    "## Using a custom segmentation dataset\n",
    "\n",
    "We will use a segmenation dataset with coffee beans, this is a FiftyOne Dataset. pjramg/my_colombian_coffe_FO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73522a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo # base library and app\n",
    "import fiftyone.utils.huggingface as fouh # Hugging Face integration\n",
    "dataset_ = fouh.load_from_hub(\"pjramg/my_colombian_coffe_FO\", persistent=True, overwrite=True)\n",
    "\n",
    "# Define the new dataset name\n",
    "dataset_name = \"coffee_FO_SAM2\"\n",
    "\n",
    "# Check if the dataset exists\n",
    "if dataset_name in fo.list_datasets():\n",
    "    print(f\"Dataset '{dataset_name}' exists. Loading...\")\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_name}' does not exist. Creating a new one...\")\n",
    "    # Clone the dataset with a new name and make it persistent\n",
    "    dataset = dataset_.clone(dataset_name, persistent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919018bc",
   "metadata": {},
   "source": [
    "## Prompting with ground truth information in the 100 unique samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_similarity(dataset, brain_key=\"img_sim2\")\n",
    "results.find_unique(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_view = dataset.select(results.unique_ids)\n",
    "session.view = unique_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b422bf",
   "metadata": {},
   "source": [
    "## Apply SAM2 just the 100 unique samples\n",
    "\n",
    "SAM 2 can also segment entire images without needing any bounding boxes or keypoints. This zero-input mode is useful for generating segmentation masks for general visual analysis or bootstrapping annotation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f026eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "\n",
    "# Full automatic segmentations\n",
    "unique_view.apply_model(model, label_field=\"sam2_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0849ae",
   "metadata": {},
   "source": [
    "In case you run out of memory, you can free up GPU space by clearing the cache with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec8530",
   "metadata": {},
   "source": [
    "## Bonus with SAM2\n",
    "\n",
    "### Prompting with Keypoints\n",
    "\n",
    "Keypoint prompts are a great alternative to bounding boxes when working with articulated objects like people. Here, we filter images to include only people, generate keypoints using a keypoint model, and then use those keypoints to prompt SAM 2 for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd660e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Filter persons only\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "dataset = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"person\")\n",
    "\n",
    "# Apply keypoint detection\n",
    "kp_model = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n",
    "dataset.default_skeleton = kp_model.skeleton\n",
    "dataset.apply_model(kp_model, label_field=\"gt_keypoints\")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SAM 2 with keypoints\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "dataset.apply_model(model, label_field=\"segmentations\", prompt_field=\"gt_keypoints_keypoints\")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2ef91",
   "metadata": {},
   "source": [
    "## Using SAM 2 for Video\n",
    "\n",
    "SAM 2 brings game-changing capabilities to video understanding. It can track segmentations across frames from a single bounding box or keypoint prompt provided on the first frame. With this, you can propagate high-quality segmentation masks through entire sequences automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Remove boxes after first frame\n",
    "(\n",
    "    dataset\n",
    "    .match_frames(F(\"frame_number\") > 1)\n",
    "    .set_field(\"frames.detections\", None)\n",
    "    .save()\n",
    ")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply video model with first-frame prompt\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-video-torch\")\n",
    "dataset.apply_model(model, label_field=\"segmentations\", prompt_field=\"frames.detections\")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bc49c",
   "metadata": {},
   "source": [
    "Available SAM 2 Models in FiftyOne\n",
    "\n",
    "Image Models:\n",
    "- segment-anything-2-hiera-tiny-image-torch\n",
    "- segment-anything-2-hiera-small-image-torch\n",
    "- segment-anything-2-hiera-base-plus-image-torch\n",
    "- segment-anything-2-hiera-large-image-torch\n",
    "\n",
    "Video Models:\n",
    "- segment-anything-2-hiera-tiny-video-torch\n",
    "- segment-anything-2-hiera-small-video-torch\n",
    "- segment-anything-2-hiera-base-plus-video-torch\n",
    "- segment-anything-2-hiera-large-video-torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voxel51",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
